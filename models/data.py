import random
from typing import List, Dict, Any, Tuple
import torch
from torch.utils.data import Dataset
from transformers import PreTrainedTokenizer

# Assuming spacy is installed
import spacy


class EntityMasker:
    """Applies selective masking to text, targeting specific entities.

    This utility is used to corrupt the input for the Composer (Decoder),
    forcing the model to rely on the latent representations generated by the
    Profiler (Encoder) and Extruder (Bridge).
    """

    def __init__(self, model_name: str = "en_core_web_sm", mask_token: str = "[MASK]", mask_prob: float = 0.4):
        try:
            self.nlp = spacy.load(model_name)
        except OSError:
            raise OSError(f"Spacy model '{model_name}' not found. Please download it.")

        self.mask_token = mask_token
        self.mask_prob = mask_prob

    def __call__(self, text: str) -> str:
        doc = self.nlp(text)
        tokens = []
        for token in doc:
            is_candidate = token.pos_ in ["PROPN", "NUM", "NOUN"]
            if is_candidate and random.random() < self.mask_prob:
                tokens.append(self.mask_token)
            else:
                tokens.append(token.text)
        return " ".join(tokens)


class PECDataset(Dataset):
    """Dataset wrapper for the PEC (Profiler-Extruder-Composer) architecture.

    Prepares dual streams of data:
    1. Clean Stream (for Profiler): Raw text for context encoding.
    2. Noisy Stream (for Composer): Masked text and Instructions for generation.
    """

    def __init__(self, data: List[Dict[str, str]], context_masker: EntityMasker, query_masker: EntityMasker):
        """Initializes the PEC dataset.

        Args:
            data: List of dicts with 'question', 'context', 'answer'.
            context_masker: EntityMasker instance to corrupt context for the Composer.
            query_masker: EntityMasker instance to corrupt query for the Composer.
        """
        self.data = data
        self.context_masker = context_masker
        self.query_masker = query_masker

        # Qwen-based Composer Chat Template
        self.user_format = "<|im_start|>user\n{query}\n\nContext:\n{context}<|im_end|>\n<|im_start|>assistant\n"
        self.eos_token = "<|im_end|>"

    def __len__(self) -> int:
        return len(self.data)

    def __getitem__(self, idx: int) -> Dict[str, str]:
        """Retrieves an item and formats it for Profiler and Composer.

        Returns:
            Dictionary containing:
            - 'profiler_input_text': Clean input for the Profiler (Modern-BERT).
            - 'composer_prompt_text': Instruction part for the Composer (for label masking).
            - 'composer_full_text': Full sequence for the Composer (Qwen).
        """
        item = self.data[idx]
        query = item.get('question', '')
        context = item.get('context', '')
        answer = item.get('answer', '')

        # 1. Input for Profiler (Clean Context)
        # The Profiler needs full visibility to generate accurate latent vectors.
        profiler_input_text = f"Question: {query} Context: {context}"

        # 2. Input for Composer (Corrupted Context via Masking)
        # The Composer receives masked context to force reliance on the Extruder's output.
        masked_context = self.context_masker(context)

        masked_query = self.query_masker(query)

        composer_prompt_text = self.user_format.format(query=masked_query, context=masked_context)
        composer_full_text = composer_prompt_text + answer + self.eos_token

        return {
            "profiler_input_text": profiler_input_text,
            "composer_prompt_text": composer_prompt_text,
            "composer_full_text": composer_full_text
        }


class PECCollator:
    """Data collator for the PEC model.

    Tokenizes inputs for both the Profiler (Encoder) and Composer (Decoder),
    and handles label masking for the Composer's causal language modeling task.
    """

    def __init__(
            self,
            profiler_tokenizer: PreTrainedTokenizer,
            composer_tokenizer: PreTrainedTokenizer,
            max_profiler_len: int = 8192,
            max_composer_len: int = 8192
    ):
        self.profiler_tokenizer = profiler_tokenizer
        self.composer_tokenizer = composer_tokenizer
        self.max_profiler_len = max_profiler_len
        self.max_composer_len = max_composer_len

    def __call__(self, batch: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:
        """Batches and tokenizes data for the PEC architecture.

        Returns:
            Dictionary containing input_ids and attention_masks for both
            Profiler and Composer, along with training labels.
        """
        # 1. Process Profiler Inputs (Modern-BERT)
        profiler_texts = [item['profiler_input_text'] for item in batch]
        profiler_inputs = self.profiler_tokenizer(
            profiler_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.max_profiler_len
        )

        # 2. Process Composer Inputs (Qwen)
        composer_full_texts = [item['composer_full_text'] for item in batch]
        composer_prompts = [item['composer_prompt_text'] for item in batch]

        composer_inputs = self.composer_tokenizer(
            composer_full_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.max_composer_len
        )

        input_ids = composer_inputs['input_ids']
        attention_mask = composer_inputs['attention_mask']

        # 3. Create Labels for Composer (Masking Instruction Part)
        labels = input_ids.clone()

        for i, prompt in enumerate(composer_prompts):
            # Calculate prompt length using the Composer's tokenizer
            prompt_tokens = self.composer_tokenizer(prompt, add_special_tokens=False)['input_ids']
            prompt_len = len(prompt_tokens)

            if prompt_len < labels.shape[1]:
                labels[i, :prompt_len] = -100

        # Ignore padding tokens in loss calculation
        labels[input_ids == self.composer_tokenizer.pad_token_id] = -100

        return {
            "profiler_input_ids": profiler_inputs['input_ids'],
            "profiler_attention_mask": profiler_inputs['attention_mask'],
            "composer_input_ids": input_ids,
            "composer_attention_mask": attention_mask,
            "labels": labels
        }